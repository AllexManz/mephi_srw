# Model Configuration
model:
  name: "gpt2"  # Базовая модель для fine-tuning
  # Альтернативные варианты:
  # - "gpt2-medium"
  # - "gpt2-large"
  # - "gpt2-xl"
  # - "facebook/opt-125m"
  # - "facebook/opt-350m"
  # - "facebook/opt-1.3b"
  # - "mistralai/Mistral-7B-v0.1"

  # Базовые настройки модели
  torch_dtype: "float16"  # Тип данных для модели
  device_map: "auto"  # Автоматическое распределение по устройствам
  trust_remote_code: true  # Доверять ли удаленному коду

  # Настройки загрузки модели
  load_in_8bit: false  # Загружать ли модель в 8-bit
  load_in_4bit: false  # Загружать ли модель в 4-bit
  quantization_config: null  # Конфигурация квантизации (если нужна)

  # Настройки LoRA
  use_lora: true  # Использовать ли LoRA для fine-tuning
  lora:
    r: 8  # Ранг адаптера
    alpha: 16  # Альфа-параметр для масштабирования
    target_modules: ["c_attn", "c_proj"]  # Модули для адаптации
    lora_dropout: 0.1  # Dropout для LoRA слоев
    bias: "none"  # Тип bias для LoRA

  # Настройки quantization
  use_8bit: true  # Использовать ли 8-bit quantization 