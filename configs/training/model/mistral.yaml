# Mistral model configuration
model:
  name: mistralai/Mistral-7B-v0.1
  torch_dtype: float16
  device_map: auto
  trust_remote_code: true
  use_cache: false  # Required for gradient checkpointing
  load_in_8bit: false
  load_in_4bit: false

# Model-specific training settings
training:
  # Mistral-specific training parameters
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  max_length: 512  # Mistral's context window 