# PEFT Configuration
method: "lora"  # Доступные методы: lora, qlora, prefix_tuning, prompt_tuning
enabled: true  # Включить ли PEFT

# Общие настройки для всех методов
common:
  task_type: "CAUSAL_LM"
  inference_mode: false

# Настройки LoRA
lora:
  r: 8  # Ранг адаптера
  alpha: 16  # Альфа-параметр для масштабирования
  target_modules: ["c_attn", "c_proj"]  # Модули для адаптации
  lora_dropout: 0.1  # Dropout для LoRA слоев
  bias: "none"  # Тип bias для LoRA
  modules_to_save: null  # Дополнительные модули для сохранения

# Настройки QLoRA
qlora:
  r: 8
  alpha: 16
  target_modules: ["c_attn", "c_proj"]
  lora_dropout: 0.1
  bias: "none"
  modules_to_save: null
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true

# Настройки Prefix Tuning
prefix_tuning:
  num_virtual_tokens: 20  # Количество виртуальных токенов
  encoder_hidden_size: 512  # Размер скрытого слоя энкодера
  prefix_projection: true  # Использовать ли проекцию префикса

# Настройки Prompt Tuning
prompt_tuning:
  num_virtual_tokens: 20
  prompt_tuning_init: "text"  # Инициализация: text или random
  token_dim: 768  # Размерность токенов
  prompt_tuning_init_text: "Classify if the text is about security:"  # Текст для инициализации 