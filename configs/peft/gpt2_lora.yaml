# GPT-2 specific LoRA configuration
defaults:
  - base

peft:
  enabled: true
  method: lora

  # GPT-2 specific LoRA settings
  lora:
    target_modules: ["c_attn", "c_proj"]  # GPT-2 specific attention modules
    r: 8
    alpha: 16
    lora_dropout: 0.1
    bias: none
    modules_to_save: null 